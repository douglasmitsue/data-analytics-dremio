# Editar os arquivos do hadoop

$ sudo apt-get install ssh -y
$ sudo apt-get install pdsh -y

# editar arquivo etc/hadoop/hadoop-env.sh
/opt/hadoop/etc/hadoop

# editar etc/hadoop/core-site.xml

<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value> -- não posso usar o localhost não funciona ao conectar no Dremio, preciso configurar o ip.
    </property>
</configuration>

# editar etc/hadoop/hdfs-site.xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>

# configurando o ip
ifconfig - para visualizar o ip da máquina
hdfs://172.17.0.1:9000

# verificar se o ssh está instalado
ssh localhost

# criando o ssh sem senha para o hadoop

$ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
$ chmod 0600 ~/.ssh/authorized_keys

# testar novamente o ssh para ver se pede a senha
ssh localhost
exit

# formatando o namenode
hdfs namenode -format

# Inicializando o hdfs
start-dfs.sh -- Permission denied -- preciso configurar uma variavel de ambiente(até a ultima versao nao precisava)
Problema no protocolo pdsh

# incluir na variável de ambiente
export PDSH_RCMD_TYPE=ssh


# Preciso configurar o YARN porque o Dremio se conecta através do YARN
start-yarn.sh

-- na prática já temos um cluster pseudo distribuído - cluster de uma máquina só
# confirmando os serviços
jps(precisa aparecer 5 serviços)
/opt/hadoop/logs -- procurar por erros caso tiver

# Verificando se o datalake esta funcionando
# verificando o conteúdo do diretório raiz
hdfs dfs -ls /
# criando o diretorio
hdfs dfs -mkdir /projetoDremio

# criando arquivo para colocar no datalake
gedit teste.txt (escrevendo dentro teste)
hdfs dfs -put teste.txt /projetoDremio

# Parar o hdfs para não corromper os arquivos
stop-dfs.sh
stop-yarn.sh

# Se der algum erro ao iniciar o hadoop - solução encontrada for formatar e copiar os arquivos novamente
hdfs namenode -format


